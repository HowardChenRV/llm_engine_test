engine: sglang
device: nvidia
use_docker: True

parentdir: /share/datasets/public_models/
models:
  # 非量化模型-llama2
  - id: llama-2-7b-chat
    tp: 1
    subdir: Llama-2-7b-chat-hf
    blocks: 2048
    max_model_len: 4096
  - id: llama-2-70b-chat
    tp: 4
    subdir: Llama-2-70b-chat-hf
    blocks: 2048
    max_model_len: 4096
  # 非量化模型-llama3
  - id: llama-3-8b-instruct
    tp: 1
    subdir: Meta-Llama-3-8B-Instruct
    blocks: 2048
    max_model_len: 8192
  - id: llama-3-70b-instruct
    tp: 4
    subdir: Meta-Llama-3-70B-Instruct-hf
    blocks: 2048
    max_model_len: 8192
  # 非量化模型-llama3.1
  - id: llama-3.1-8b-instruct
    tp: 1
    subdir: Meta-Llama-3.1-8B-Instruct
    blocks: 2048
    max_model_len: 32768
  - id: llama-3.1-70b-instruct
    tp: 4
    subdir: Meta-Llama-3.1-70B-Instruct
    blocks: 2048
    max_model_len: 32768
  # 非量化模型-qwen1.5
  - id: qwen-1.5-4b-chat
    tp: 1
    subdir: Qwen_Qwen1.5-4B-Chat
    blocks: 2048
    max_model_len: 32768
  - id: qwen-1.5-7b-chat
    tp: 1
    subdir: Qwen_Qwen1.5-7B-Chat
    blocks: 2048
    max_model_len: 32768
  - id: qwen-1.5-14b-chat
    tp: 1
    subdir: Qwen_Qwen1.5-14B-Chat
    blocks: 2048
    max_model_len: 32768
  - id: qwen-1.5-32b-chat
    tp: 2
    subdir: Qwen_Qwen1.5-32B-Chat
    blocks: 2048
    max_model_len: 32768
  - id: qwen-1.5-72b-chat
    tp: 4
    subdir: Qwen_Qwen1.5-72B-Chat
    blocks: 2048
    max_model_len: 32768
  # 非量化模型-qwen2
  - id: qwen-2-7b-instruct
    tp: 1
    subdir: Qwen_Qwen2-7B-Instruct
    blocks: 2048
    max_model_len: 32768
  - id: qwen-2-72b-instruct
    tp: 4
    subdir: Qwen_Qwen2-72B-Instruct
    blocks: 2048
    max_model_len: 32768
  # 非量化模型-qwen2.5
  - id: qwen-2.5-7b-instruct
    tp: 1
    subdir: Qwen_Qwen2.5-7B-Instruct
    blocks: 2048
    max_model_len: 32768
  - id: qwen-2.5-14b-instruct
    tp: 1
    subdir: Qwen_Qwen2.5-14B-Instruct
    blocks: 2048
    max_model_len: 32768
  - id: qwen-2.5-72b-instruct
    tp: 4
    subdir: Qwen_Qwen2.5-72B-Instruct
    blocks: 2048
    max_model_len: 32768
  - id: qwen-2.5-coder-32b-instruct
    tp: 2
    subdir: Qwen2.5-Coder-32B-Instruct
    blocks: 18000
    max_model_len: 32768
    fim_template: qwen2.5-coder
  # 量化模型-fp8
  - id: qwen-1.5-72b-chat-fp8
    tp: 2
    subdir: /share/datasets/tmp_share/chenyonghua/qwen1.5-72B-chat-llmcompressor-fp8
    path: /share/datasets/tmp_share/chenyonghua/qwen1.5-72B-chat-llmcompressor-fp8
    blocks: 1800
    max_model_len: 25600
    quantization_method: fp8
  - id: qwen-1.5-7b-chat-fp8
    tp: 1
    subdir: /share/datasets/tmp_share/chenyonghua/qwen1.5-7B-chat-llmcompressor-fp8
    path: /share/datasets/tmp_share/chenyonghua/qwen1.5-7B-chat-llmcompressor-fp8
    blocks: 1800
    max_model_len: 25600
    quantization_method: fp8
  - id: qwen-2-72b-instruct-fp8
    tp: 2
    subdir: /share/datasets/tmp_share/chenyonghua/qwen2-72B-instruct-llmcompressor-fp8
    path: /share/datasets/tmp_share/chenyonghua/qwen2-72B-instruct-llmcompressor-fp8
    blocks: 1800
    max_model_len: 25600
    quantization_method: fp8
  - id: qwen-2.5-14b-instruct-fp8
    tp: 1
    subdir: /share/datasets/tmp_share/chenyonghua/qwen2.5-14B-instruct-llmcompressor-fp8
    path: /share/datasets/tmp_share/chenyonghua/qwen2.5-14B-instruct-llmcompressor-fp8
    blocks: 2048
    max_model_len: 25600
    quantization_method: fp8
  # 量化模型-awq
  - id: qwen-2.5-32b-instruct-awq
    tp: 1
    subdir: Qwen_Qwen2.5-32B-Instruct-AWQ
    blocks: 2048
    max_model_len: 32768
    quantization_method: awq
  - id: qwen-2-72b-instruct-awq
    tp: 2
    subdir: Qwen_Qwen2-72B-Instruct-AWQ
    blocks: 2048
    max_model_len: 32768
    quantization_method: awq
  # 量化模型-deepseek A1上没模型
  # - id: deepseek-2.5-coder-236b
  #   tp: 8
  #   subdir: DeepSeek-V2.5-AWQ
  #   path: /mnt/public/DeepSeek-V2.5-AWQ
  #   blocks: 3600
  #   max_model_len: 32768
  #   quantization_method: awq
  #   fim_template: deepseek
  # prefix caching
  - id: qwen-2-7b-instruct-prefix_caching
    tp: 1
    subdir: Qwen_Qwen2-7B-Instruct
    blocks: 2048
    max_model_len: 32768
    enable_prefix_caching: True
  - id: qwen-2.5-14b-instruct-prefix_caching
    tp: 2
    subdir: Qwen_Qwen2.5-14B-Instruct
    blocks: 2048
    max_model_len: 32768
    enable_prefix_caching: True